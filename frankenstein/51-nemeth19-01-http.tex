\chapter{Hypertext Transfer Protocol}
\label{chap:http}

HTTP is the core network protocol for communication on the web. Lurking
beneath a deceptively simple facade of stateless requests and responses
lie layers of refinements that bring both flexibility and complexity. A
well-rounded understanding of HTTP is a core competency for all system
administrators.

In its simplest form, HTTP is a client/server, one-request/one-response
protocol. Clients, also called user agents, submit requests for
resources to an HTTP server. Servers receive incoming requests and
process them by retrieving files from local disks, resubmitting them to
other servers, querying databases, or performing any number of other
possible computations. A typical page view on the web entails dozens or
hundreds of such exchanges.

As with most Internet protocols, HTTP has adapted over time, albeit
slowly. The centrality of the protocol to the modern Internet makes
updates a high-stakes proposition. Official revisions are a slog of
committee meetings, mailing list negotiations, public review periods,
and maneuvering by stakeholders with vested and conflicting interests.
During the long gaps between official revisions documented in RFCs,
unofficial protocol extensions are born from necessity, become
ubiquitous, and are eventually included as features in the next
specification.

\protect\hypertarget{part0027_split_001.htmlux5cux23_idIndexMarker2745}{}{}HTTP
versions 1.0 and 1.1 are sent over the wire in plain text. Adventurous
administrators can interact with servers directly by running {telnet} or
\protect\hypertarget{part0027_split_001.htmlux5cux23_idIndexMarker2746}{}{}{netcat}.
They can also observe and collect HTTP exchanges by using
protocol-agnostic packet capture software such as
\protect\hypertarget{part0027_split_001.htmlux5cux23_idIndexMarker2747}{}{}{tcpdump}.

\leavevmode\hypertarget{part0027_split_001.htmlux5cux23_idContainer1250}{}%
See
\protect\hyperlink{part0037_split_040.htmlux5cux23_idTextAnchor1727}{this
page} for general information about TLS.

The web is in the process of adopting HTTP/2, a major protocol revision
that preserves compatibility with previous versions but introduces a
variety of performance improvements. In an effort to promote the
universal use of HTTPS (secure, encrypted HTTP) for the next generation
of the web, major browsers such as Firefox and Chrome have elected to
support HTTP/2 only over TLS-encrypted connections.

HTTP/2 moves from plain text to binary format in an effort to simplify
parsing and improve network efficiency. HTTP's semantics remain the
same, but because the transmitted data is no longer directly legible to
humans, generic tools such as {telnet} are no longer useful. The handy
\protect\hypertarget{part0027_split_001.htmlux5cux23_idIndexMarker2748}{}{}\protect\hypertarget{part0027_split_001.htmlux5cux23_idIndexMarker2749}{}{}{h2i}
command-line utility, part of the Go language networking repository at
\href{http://github.com/golang/net}{github.com/golang/net}, helps
restore some interactivity and debuggability to HTTP/2 connections. Many
HTTP-specific tools such as
\protect\hypertarget{part0027_split_001.htmlux5cux23_idIndexMarker2750}{}{}{curl}
also support HTTP/2 natively.


\section{Uniform resource locators (URLs)}


\protect\hypertarget{part0027_split_002.htmlux5cux23_idIndexMarker2751}{}{}\protect\hypertarget{part0027_split_002.htmlux5cux23_idIndexMarker2752}{}{}A
URL is an identifier that specifies how and where to access a resource.
URLs are not HTTP-specific; they are used for other protocols as well.
For example, mobile operating systems use URLs to facilitate
communication among apps.

You may sometimes see the acronyms
\protect\hypertarget{part0027_split_002.htmlux5cux23_idIndexMarker2753}{}{}URI
(Uniform Resource Identifier) and
\protect\hypertarget{part0027_split_002.htmlux5cux23_idIndexMarker2754}{}{}URN
(Uniform Resource Name) used as well. The exact distinctions and
taxonomic relationships among URLs, URIs, and URNs are vague and
unimportant. Stick with ``URL.''

The general pattern for URLs is {scheme:address}, where {scheme}
identifies the protocol or system being targeted and {address} is some
string that's meaningful within that scheme. For example, the URL
mailto:ulsah@admin.com encapsulates an email address. If it's invoked as
a link target on the web, most browsers will bring up a preaddressed
window for sending mail.

For the web, the relevant schemes are http and https. In the wild, you
might also see the schemes ws (WebSockets), wss (WebSockets over TLS),
ftp, ldap, and many others.

The address portion of a web URL allows quite a bit of interior
structure. Here's the overall pattern:

{}{scheme}://{[}{username:password}@{]}{hostname}{[}:{port}{]}{[}/{path}{]}{[}?{query}{]}{[}\#{anchor}{]}

All the elements are optional except {scheme} and {hostname}.

\leavevmode\hypertarget{part0027_split_002.htmlux5cux23_idContainer1251}{}%
See
\protect\hyperlink{part0027_split_023.htmlux5cux23_idTextAnchor1257}{this
page} for more details about HTTP basic authentication.

\protect\hypertarget{part0027_split_002.htmlux5cux23_idIndexMarker2755}{}{}The
use of a {username} and {password} in the URL enables ``HTTP basic
authentication,'' which is supported by most user agents and servers. In
general, it's a bad idea to embed passwords into URLs because URLs are
apt to be logged, shared, bookmarked, visible in {ps} output, etc. User
agents can get their credentials from a source other than the URL, and
that is typically a better option. In a web browser, just leave the
credentials out and let the browser prompt you for them separately.

HTTP basic authentication is not self-securing, which means that the
password is accessible to anyone who listens in on the transaction.
Therefore, basic authentication should really only be used over secure
\protect\hypertarget{part0027_split_002.htmlux5cux23_idIndexMarker2756}{}{}HTTPS
connections.

\protect\hypertarget{part0027_split_002.htmlux5cux23_idIndexMarker2757}{}{}The
{hostname} can be a domain name or IP address as well as an actual
hostname. The {port} is the TCP port number to connect to. The http and
https schemes default to ports 80 and 443, respectively.

The {query} section can include multiple parameters separated by
ampersands. Each parameter is a {key=value} pair. For example,
\protect\hypertarget{part0027_split_002.htmlux5cux23_idIndexMarker2758}{}{}Adobe
InDesign users may find the following URL eerily familiar:

{}http://adobe.com/search/index.cfm?term=indesign+crash\&loc=en\_us

\protect\hypertarget{part0027_split_002.htmlux5cux23_idIndexMarker2759}{}{}As
with passwords, sensitive data should never appear as a URL query
parameter because URL paths are often logged as plain text. The
alternative is to transmit parameters as part of the request body. (You
can't really control this in other people's web software, but you can
make sure your own site behaves properly.)

The {anchor} component identifies a subtarget of a specific URL. For
example, Wikipedia uses named anchors extensively as section headings,
allowing specific parts of an entry to be linked to directly.


\section{Structure of an HTTP transaction}

HTTP requests and responses are similar in structure. After an initial
line, both include a sequence of headers, a blank line, and finally, the
body of the message, called the payload.

\subsubsection[HTTP
requests]{\texorpdfstring{\protect\hypertarget{part0027_split_003.htmlux5cux23_idTextAnchor1211}{}{}HTTP
requests}{HTTP requests}}

The first line of a request specifies an action for the server to
perform. It consists of a request method (also known as the verb), a
path on which to perform the action, and the HTTP version to use. For
example, a request to retrieve a top-level HTML page might look like
this:

%\includegraphics{images/00919.gif}

\protect\hyperlink{part0027_split_003.htmlux5cux23_idTextAnchor1212}{Table
19.1} shows the common HTTP request methods. Verbs marked as ``safe''
should not change the server's state. However, this is more a convention
than a mandate. It's ultimately up to the software that handles the
request to decide how to interpret the verb.

\paragraph[{Table 19.1: }HTTP request methods]{\texorpdfstring{{Table
19.1:
}\protect\hypertarget{part0027_split_003.htmlux5cux23_idIndexMarker2760}{}{}\protect\hypertarget{part0027_split_003.htmlux5cux23_idTextAnchor1212}{}{}HTTP
request methods}{Table 19.1: HTTP request methods}}

%\includegraphics{images/00920.gif}

GET is by far the most commonly used HTTP verb, followed by POST. REST
APIs, discussed in
\protect\hyperlink{part0027_split_014.htmlux5cux23_idTextAnchor1244}{{Application
programming interfaces (APIs)}}, are more likely to employ the more
exotic verbs such as PUT and DELETE.

The distinction between POST and PUT is subtle and largely of concern to
web API developers. PUTs should be idempotent, meaning that a PUT can be
repeated without causing ill effects. For example, a transaction that
causes the server to send email should not be represented as a PUT. The
rules for HTTP caching also differ significantly between PUT and POST.
See RFC2616 for more details.

\subsubsection[HTTP
responses]{\texorpdfstring{\protect\hypertarget{part0027_split_003.htmlux5cux23_idTextAnchor1213}{}{}HTTP
responses}{HTTP responses}}

\protect\hypertarget{part0027_split_003.htmlux5cux23_idIndexMarker2761}{}{}The
initial line in a response, called the status line, indicates the
disposition of the request. It looks like this:

%\includegraphics{images/00921.gif}

The important part is the three-digit numeric status code. The phrase
that follows it is a helpful English translation that software ignores.

The first digit in the code determines its class; that is, the general
nature of the result.
\protect\hyperlink{part0027_split_003.htmlux5cux23_idTextAnchor1214}{Table
19.2} shows the five defined classes. Within a class, additional detail
is provided by the remaining two digits. More than 60 status codes are
defined, but only a few of these are commonly seen in the wild.

\paragraph[{Table 19.2: }HTTP response classes]{\texorpdfstring{{Table
19.2:
}\protect\hypertarget{part0027_split_003.htmlux5cux23_idTextAnchor1214}{}{}HTTP
response classes}{Table 19.2: HTTP response classes}}

%\includegraphics{images/00922.gif}

\subsubsection[Headers and the message
body]{\texorpdfstring{\protect\hypertarget{part0027_split_003.htmlux5cux23_idTextAnchor1215}{}{}Headers
and the message body}{Headers and the message body}}

Headers specify metadata about a request or response, such as whether to
allow compression; what types of content are accepted, expected, or
provided; and how intermediate caches should handle the data. For
requests, the only required header is Host, which is used by web server
software to determine which site is being contacted.

\protect\hyperlink{part0027_split_003.htmlux5cux23_idTextAnchor1216}{Table
19.3} shows some common headers.

\paragraph[{Table 19.3: }Commonly encountered HTTP
headers]{\texorpdfstring{{Table 19.3:
}\protect\hypertarget{part0027_split_003.htmlux5cux23_idIndexMarker2762}{}{}\protect\hypertarget{part0027_split_003.htmlux5cux23_idTextAnchor1216}{}{}Commonly
encountered HTTP
headers}{Table 19.3: Commonly encountered HTTP headers}}

%\includegraphics{images/00923.gif}

\protect\hyperlink{part0027_split_003.htmlux5cux23_idTextAnchor1216}{Table
19.3} is by no means a definitive list. In fact, both sides of the
transaction can include any headers they wish. Both sides must ignore
headers they don't understand. By convention, custom and experimental
headers were originally prefixed with ``X-''. But some X- headers (such
as X-Forwarded-For) became de facto standards, and it was then
infeasible to remove the prefix because that would break compatibility.
The use of X- is now deprecated by RFC6648.

Headers are separated from the message body by a blank line. For
requests, the body can include parameters (for POST or PUT requests) or
the contents of a file to upload. For responses, the message body is the
payload of the resource being requested (e.g., HTML, image data, or
query results). The message body is not necessarily human-readable,
since it can contain images or other binary data. The body can also be
empty, as for GET requests or most error responses.


\section{curl: HTTP from the command line}


\protect\hypertarget{part0027_split_004.htmlux5cux23_idIndexMarker2763}{}{}\protect\hypertarget{part0027_split_004.htmlux5cux23_idIndexMarker2764}{}{}{curl}
(cURL) is a handy command-line HTTP client that's available for most
platforms. (Administrators will also encounter {libcurl}, a client
library that developers can use to give their own software {curl}-like
superpowers.) Here, we use {curl} to explore an HTTP exchange.

Below is an invocation of {curl} that requests the root of the web site
{admin.com} on TCP port 80, which is the default for unencrypted
(non-HTTPS) requests. The response payload (i.e., the admin.com
homepage) and some informative messages from {curl} itself have been
hidden by the {-o /dev/null} and {-s} flags. We also include the {-v}
flag to request that {curl} display verbose output, which includes
headers.

%\includegraphics{images/00924.gif}

Lines starting with \textgreater{} and \textless{} denote the request
and response, respectively. In the request, the client tells the server
that the user agent is {curl}, that it's looking for host admin.com, and
that it will accept any type of content as a response. The server
identifies itself as Apache 2.4.7 and replies with contents of type
HTML, along with a variety of other metadata.

We can set headers explicitly with {curl}'s {-H} argument. This feature
is especially handy for making requests directly against IP addresses,
bypassing DNS. For example, we could check that the server for
www.admin.com responds identically to requests targeted at admin.com by
setting the Host header, which informs the remote server of the domain
the user agent is attempting to contact:

%\includegraphics{images/00925.gif}

We use the {-O} argument to download a file. This example downloads a
tarball of the {curl} source code to the current directory:

%\includegraphics{images/00926.gif}

We've only scratched the surface of {curl}'s capabilities. It can handle
other request methods such as POST and DELETE, store and submit cookies,
download files, and assist with many different debugging scenarios.

Google's Chrome browser offers a feature called ``Copy as cURL'' that
creates a {curl} command to simulate the browser's own behavior,
including headers, cookies, and other details. You can easily retry
requests with various adjustments and see the results exactly as the
browser would. (Right-click a resource name in the Network tab of the
developer tools panel to uncover this option.)


\section{TCP connection reuse}

\protect\hypertarget{part0027_split_005.htmlux5cux23_idIndexMarker2765}{}{}\protect\hypertarget{part0027_split_005.htmlux5cux23_idIndexMarker2766}{}{}TCP
connections are expensive. In addition to the memory needed to maintain
them, the three-way handshake used to establish each new connection adds
latency equivalent to a full round trip before an HTTP request can even
begin.
(\protect\hypertarget{part0027_split_005.htmlux5cux23_idIndexMarker2767}{}{}\protect\hypertarget{part0027_split_005.htmlux5cux23_idIndexMarker2768}{}{}TCP
Fast Open is a proposal that aims to improve this situation by allowing
the SYN and SYN-ACK packets of TCP's three-way handshake to also carry
data. See RFC7413.)

The HTTP Archive, a project that tracks web statistics, estimates that
the average site incurs requests for 99 resources per page load. If each
resource required a new TCP connection, the performance of the web would
be atrocious indeed. This was in fact the case early in the life of the
web.

The original HTTP/1.0 specification did not include any provisions for
connection reuse, but some adventurous developers added experimental
support as an extension. The Connection:
\protect\hypertarget{part0027_split_005.htmlux5cux23_idIndexMarker2769}{}{}Keep-Alive
header was added informally to clients and servers, then improved and
made the default in HTTP/1.1. With keep-alive (also known as persistent)
connections, HTTP clients and servers send multiple requests over a
single connection, thus saving some of the cost and latency of
initiating and tearing down multiple connections.

TCP overhead turns out to be nontrivial even when HTTP/1.1 persistent
connections are enabled. Most browsers open as many as six parallel
connections to the server to improve performance. Busy servers in turn
must maintain many thousands of TCP connections in various states,
resulting in network congestion and wasted resources.

HTTP/2 introduces multiplexing as a solution, allowing several
transactions to be interleaved on a single connection. HTTP/2 servers
can therefore support more clients per system, since each client imposes
lower overhead.



\section{HTTP over TLS}

\protect\hypertarget{part0027_split_006.htmlux5cux23_idIndexMarker2770}{}{}On
its own, HTTP provides no network-level security. The URL, headers, and
payload are open to inspection and modification at any point between the
client and server. A malevolent infiltrator can intercept messages,
alter their contents, or redirect requests to servers of its choice.

Enter
\protect\hypertarget{part0027_split_006.htmlux5cux23_idIndexMarker2771}{}{}\protect\hypertarget{part0027_split_006.htmlux5cux23_idIndexMarker2772}{}{}\protect\hypertarget{part0027_split_006.htmlux5cux23_idIndexMarker2773}{}{}Transport
Layer Security (TLS), which runs as a separate layer between TCP and
HTTP. TLS supplies only the security and encryption for the connection;
it does not involve itself at the HTTP layer.

The precursor of TLS was known as
\protect\hypertarget{part0027_split_006.htmlux5cux23_idIndexMarker2774}{}{}\protect\hypertarget{part0027_split_006.htmlux5cux23_idIndexMarker2775}{}{}SSL,
the Secure Sockets Layer. All versions of SSL are obsolete and formally
deprecated, but the name SSL remains in wide colloquial use. Outside of
cryptographic contexts, assume that references to SSL really mean TLS.

The user agent verifies the server's identity as part of the TLS
connection process, eliminating the possibility of spoofing by
counterfeit servers. Once the connection is established, its contents
are protected against snooping and modification for the duration of the
exchange. Attackers can still see the host and port used at the TCP
layer, but they cannot access HTTP details such as the URL of a request
or the headers that accompany it.

See
\protect\hyperlink{part0037_split_040.htmlux5cux23_idTextAnchor1727}{{Transport
Layer Security}} for more details on TLS cryptography.



\section{Virtual hosts}

\protect\hypertarget{part0027_split_007.htmlux5cux23_idIndexMarker2776}{}{}\protect\hypertarget{part0027_split_007.htmlux5cux23_idIndexMarker2777}{}{}\protect\hypertarget{part0027_split_007.htmlux5cux23_idIndexMarker2778}{}{}In
the early days of the web, a server typically hosted only a single web
site. When admin.com was requested, for example, clients performed a DNS
lookup to find the IP address associated with that name and then sent an
HTTP request to port 80 at that address. The server at that address knew
that it was dedicated to hosting admin.com and served results
accordingly.

As web use increased, administrators realized that they could achieve
economies of scale if a single server could host more than one site at
once. But how do you distinguish requests bound for admin.com from those
bound for example.com if both kinds of traffic end up at the same
network port?

One possibility is to define virtual network interfaces, effectively
permitting several different IP addresses to be bound to a single
physical connection. Most systems allow this, and it works fine, but the
scheme is fiddly and requires management at several different layers.

A better solution, virtual hosts, was provided by HTTP 1.1 in RFC2616.
This scheme defines a Host HTTP header that user agents set explicitly
to indicate what site they're attempting to contact. Servers examine the
header and behave accordingly. This convention conserves IP addresses
and simplifies management, especially for sites that have hundreds or
thousands of web sites on a single server.

HTTP 1.1 {requires} user agents to provide a Host header, so virtual
hosts are now the standard way that web servers and administrators
handle server consolidation.

The use of name-based virtual hosts in combination with TLS is a bit
tricky under the hood. TLS certificates are issued to specific
hostnames, which are chosen when the certificate is generated. A TLS
connection must be established before the server can read the Host
header from the HTTP request, but without that header, the server does
not know which virtual host it should be impersonating, and hence, which
certificate to select.

The solution is
\protect\hypertarget{part0027_split_007.htmlux5cux23_idIndexMarker2779}{}{}\protect\hypertarget{part0027_split_007.htmlux5cux23_idIndexMarker2780}{}{}SNI,
Server Name Indication, in which the client submits the hostname that
it's requesting as part of the initial TLS connection message. Modern
servers and clients all handle SNI automatically.
